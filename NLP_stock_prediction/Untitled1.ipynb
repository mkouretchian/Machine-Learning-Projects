{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/roji/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-13312af53c1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0mBEARERTOKEN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'AAAAAAAAAAAAAAAAAAAAAD5IUwEAAAAAYRmAUWOXGmKinw5EJY4c0mK4L%2Fk%3DfQ4d0x6AHSjU0HYYbya8lnLEE5QRsIyWakBJmiTVqscmtYEYHi'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0mtweeter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweeter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tsla'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBEARERTOKEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweeter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_data_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-13312af53c1e>\u001b[0m in \u001b[0;36mcreate_data_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'data'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "import flair\n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "\n",
    "\n",
    "class tweeter():\n",
    "    def __init__(self,ticker,BEARER_TOKEN,max_result):\n",
    "        self.ticker = ticker\n",
    "        self.BEARER_TOKEN = BEARER_TOKEN\n",
    "        self.max_results = str(max_result)\n",
    "        \n",
    "    def clean_tweet(self,tweet):\n",
    "        tweet = re.sub(\"@[A-Za-z0-9]+\",\"\",tweet) #Remove @ sign\n",
    "        tweet = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", tweet) #Remove http links\n",
    "        tweet = \" \".join(tweet.split())\n",
    "        tweet = ''.join(c for c in tweet if c not in emoji.UNICODE_EMOJI) #Remove Emojis\n",
    "        tweet = tweet.replace(\"#\", \"\").replace(\"_\", \" \") #Remove hashtag sign but keep the text\n",
    "        tweet = \" \".join(w for w in nltk.wordpunct_tokenize(tweet) \\\n",
    "                         if w.lower() in words or not w.isalpha())\n",
    "        return tweet\n",
    "    \n",
    "    def get_data(self,tweet):\n",
    "        data = {'id':tweet['id'],\n",
    "                'created_at':tweet['created_at'],\n",
    "                'text':tweet['text']}\n",
    "        return data\n",
    "    \n",
    "    def create_data_frame(self):\n",
    "        \n",
    "        endpoint = 'https://api.twitter.com/2/tweets/search/recent'\n",
    "        headers = {'authorization': f\"Bearer {self.BEARER_TOKEN}\"}\n",
    "        params = {\n",
    "                    'query': '(self.ticker) (lang:en)',\n",
    "                    'max_results': self.max_results,\n",
    "                    'tweet.fields': 'created_at,lang'\n",
    "                    }\n",
    "        response = requests.get(endpoint,\n",
    "                            params=params,\n",
    "                            headers=headers)\n",
    "        response.json()\n",
    "        df = pd.DataFrame()\n",
    "        for tweet in response.json()['data']:\n",
    "            row = self.get_data(tweet)\n",
    "            df.append(row , ignore_index = True)\n",
    "            \n",
    "        df['text'] = df['text'].map(lambda x : self.clean_tweet(x))\n",
    "        return df\n",
    "    \n",
    "    def adding_prob_and_sentiment(self,df):\n",
    "        probs = []\n",
    "        sentiments = []\n",
    "        sentiment_model = flair.models.TextClassifier.load('en-sentiment')\n",
    "        for tweet in df['text'].to_list():\n",
    "            sentence = flair.data.Sentence(tweet)\n",
    "            sentiment_model.predict(sentence)\n",
    "            probs.append(sentence.label[0].score)\n",
    "            sentiments.append(sentence.label[0].value)\n",
    "        df['probs'] = probs\n",
    "        df['sentiments'] = sentiments\n",
    "        \n",
    "        return df\n",
    "        \n",
    "BEARERTOKEN = 'AAAAAAAAAAAAAAAAAAAAAD5IUwEAAAAAYRmAUWOXGmKinw5EJY4c0mK4L%2Fk%3DfQ4d0x6AHSjU0HYYbya8lnLEE5QRsIyWakBJmiTVqscmtYEYHi'    \n",
    "tweeter = tweeter('tsla',BEARERTOKEN,100)\n",
    "df = tweeter.create_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
